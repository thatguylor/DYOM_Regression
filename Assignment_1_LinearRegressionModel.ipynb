{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination: -1.54\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df= pd.read_csv(\"filtered_062019.csv\",index_col=0)\n",
    "df1=df.dropna()\n",
    "df1 = df1.drop_duplicates()\n",
    "df1= df1.drop(['gpu0.mem_max', 'gpu0.smUtil_avg', 'gpu0.smUtil_max', 'gpu1.mem_max', 'gpu1.smUtil_avg', 'gpu1.smUtil_max', 'gpu2.mem_max', 'gpu2.smUtil_avg', 'gpu2.smUtil_max', 'gpu3.mem_max', 'gpu3.smUtil_avg', 'gpu3.smUtil_max'],axis=1)\n",
    "df1=df1.drop(['run_count', 'start', 'status', 'wait_time','cluster', 'ctime', 'datetime', 'end', 'etime', 'exec_host', 'exec_vnode', 'group', 'job_id', 'qtime'],axis=1)\n",
    "df1=df1.drop(['Exit_status', 'Resource_List.fluent_lic','Resource_List.mpiprocs','Resource_List.ngpus','resources_used.GPU_duration','resources_used.GPU_maxGpuMemoryUsed', 'resources_used.GPU_energyConsumed','Resource_List.nodect'],axis=1)\n",
    "df1['CPU_eff']= (df1['resources_used.cput']/df1['resources_used.walltime'])/df1['Resource_List.ncpus']\n",
    "df1 = df1.replace([np.inf, -np.inf], np.nan)\n",
    "df1=df1.dropna()\n",
    "X = df1.drop('CPU_eff', axis=1)\n",
    "y = df1['CPU_eff']\n",
    "\n",
    "\n",
    "X_encoded = pd.get_dummies(X, prefix_sep='_', drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2,random_state=0)\n",
    "\n",
    "#lab_enc = preprocessing.LabelEncoder()\n",
    "#y_train_encoded = lab_enc.fit_transform(y_train)\n",
    "#y_test_encoded = lab_enc.fit_transform(y_test)\n",
    "X_train_scaled= StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled= StandardScaler().fit_transform(X_test)\n",
    "\n",
    "\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "regressor = LinearRegression()\n",
    "regr = TransformedTargetRegressor(regressor=regressor,\n",
    "                                   transformer=transformer)\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#print('R2 score: {0:.2f}'.format(regr.score(X_test_scaled, y_test)))\n",
    "\n",
    "raw_target_regr = LinearRegression().fit(X_train_scaled, y_train)\n",
    "#print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test_scaled, y_test)))\n",
    "\n",
    "#Believe the R-Squared term is bad because features have not been scaled properly. \n",
    "\n",
    "#Predictions using test data, where 1 is perfect prediction\n",
    "cpu_predictor_y= regr.predict(X_test_scaled)\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test_encoded, cpu_predictor_y))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_encoded, cpu_predictor_y))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_encoded, cpu_predictor_y))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_encoded, cpu_predictor_y)))\n",
    "\n",
    "#Explanation of Pipeline: \n",
    "#Data was loaded in pd.read, columns with NaN were dropped immediately. As after doing EDA, the column with NaN was deemed irrelevant.\n",
    "#Next, columns with GPU related functions were also dropped because they have no physical relation with CPU usage based on intuition.\n",
    "#Thereafter, columns with zero variance were dropped, followed by redundant physical interpretations like admin status or date-time.\n",
    "#New column CPU_eff was inserted based on the formula given.\n",
    "#All values that contained inf were replaced with NaN and dropped. \n",
    "\n",
    "#With the dataset, X -> contained 12 features in total and y -> was the target variable, CPU_eff\n",
    "\n",
    "#Categorical features in X were user,dept,queue and were converted to numerical values using get_dummies from pandas \n",
    "\n",
    "#X and y were split into train and test dataset, before using label encoder to fit the target variable y. Standardscaler was then used to fit and transform X\n",
    "\n",
    "#X and y were then fed into LinearRegression() to generate a multivariable regression model and the evaluation metrics are shown below.\n",
    "\n",
    "#Analysis of coefficient of determination: This negative R-squared term implies that perhaps the model confused several instances with itself, and confused\n",
    "#the first instance with the fifth instance for example. Or perhaps the features used were not strong enough to form any predictive capability \n",
    "#Or maybe the data was not scaled properly according to the target variable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
